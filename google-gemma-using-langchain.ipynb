{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":4525,"sourceType":"datasetVersion","datasetId":2735},{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374},{"sourceId":7711309,"sourceType":"datasetVersion","datasetId":4484051},{"sourceId":7731345,"sourceType":"datasetVersion","datasetId":4517764},{"sourceId":7733314,"sourceType":"datasetVersion","datasetId":4518936},{"sourceId":7923451,"sourceType":"datasetVersion","datasetId":4633221},{"sourceId":8879216,"sourceType":"datasetVersion","datasetId":5344133},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T20:19:48.718637Z","iopub.execute_input":"2024-07-13T20:19:48.719020Z","iopub.status.idle":"2024-07-13T20:19:49.867800Z","shell.execute_reply.started":"2024-07-13T20:19:48.718986Z","shell.execute_reply":"2024-07-13T20:19:49.866675Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/questionanswer-dataset/text_data_toc.csv\n/kaggle/input/questionanswer-dataset/S10_question_answer_pairs.txt\n/kaggle/input/questionanswer-dataset/S09_question_answer_pairs.txt\n/kaggle/input/questionanswer-dataset/S08_question_answer_pairs.txt\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set3_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set2_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set5_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set3_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set5_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set1_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set4_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set3_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set6_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/S10_set2_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set4_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set1_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S10_set4_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S08_set2_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/S09_set1_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set3_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a7.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set2_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set5_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set3_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set5_a8.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a2.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a1.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set1_a3.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a6.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set4_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set3_a4.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set6_topics.txt\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set2_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set4_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set1_a10.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S10_set4_a5.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S08_set2_a9.txt.clean\n/kaggle/input/questionanswer-dataset/text_data/text_data/S09_set1_a3.txt.clean\n/kaggle/input/1000-data-science-concepts/data_science_concepts.csv\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/config.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/tokenizer.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/metadata.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/model.weights.h5\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/prompts_0_500_wiki_first_para_3000.csv\n/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/rewrite_prompts.csv\n/kaggle/input/kaggle-docs/questions_answers/data.csv\n/kaggle/input/kaggle-docs/raw/Notebooks.txt\n/kaggle/input/kaggle-docs/raw/Public API.txt\n/kaggle/input/kaggle-docs/raw/Organizations.txt\n/kaggle/input/kaggle-docs/raw/Competitions Setup.txt\n/kaggle/input/kaggle-docs/raw/Datasets.txt\n/kaggle/input/kaggle-docs/raw/Models.txt\n/kaggle/input/kaggle-docs/raw/TPU Usage.txt\n/kaggle/input/kaggle-docs/raw/GPU Usage.txt\n/kaggle/input/kaggle-docs/raw/Competitions.txt\n/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n/kaggle/input/kaggledocsquestionanswers/data.csv\n/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n/kaggle/input/llm-prompt-recovery-synthetic-datastore/gemma1000_w7b.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n#!pip install -U transformers\n!pip install plotly\n!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-07-13T20:19:49.869916Z","iopub.execute_input":"2024-07-13T20:19:49.870521Z","iopub.status.idle":"2024-07-13T20:20:46.670002Z","shell.execute_reply.started":"2024-07-13T20:19:49.870477Z","shell.execute_reply":"2024-07-13T20:20:46.668650Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.18.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly) (3.1.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2+cpu)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n\nimport keras\nimport keras_nlp\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nimport tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom IPython.display import display, Markdown","metadata":{"execution":{"iopub.status.busy":"2024-07-13T20:20:46.671748Z","iopub.execute_input":"2024-07-13T20:20:46.672112Z","iopub.status.idle":"2024-07-13T20:21:06.164472Z","shell.execute_reply.started":"2024-07-13T20:20:46.672076Z","shell.execute_reply":"2024-07-13T20:21:06.163301Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-13 20:20:50.411478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-13 20:20:50.411629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-13 20:20:50.552700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Key Configuration Parameters of Gemma models","metadata":{}},{"cell_type":"code","source":"class CFG:\n  # Set a seed value for reproducibility (e.g., same random number generation)\n  seed = 123\n  # Path to the directory containing your dataset\n  dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n  # Name of the pre-trained Gemma model to use (\"gemma_2b_en\" for English)\n  preset = \"gemma_2b_en\"\n  # Maximum length of an input sequence for training (in tokens)\n  sequence_length = 512\n  # Number of samples processed in a single training step (adjust based on hardware)\n  batch_size = 1\n  # Number of times to iterate through the entire dataset during training\n  epochs = 10\n  # Optimizer used for training the model (Adam with a learning rate of 3e-5)\n  optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n  # Loss function used during training (Sparse Categorical Crossentropy)\n  loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T20:25:39.687929Z","iopub.execute_input":"2024-07-13T20:25:39.689084Z","iopub.status.idle":"2024-07-13T20:25:40.066009Z","shell.execute_reply.started":"2024-07-13T20:25:39.689043Z","shell.execute_reply":"2024-07-13T20:25:40.064913Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\n\n# Tokenizer\ntokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\"gemma_instruct_2b_en\")\n# Model Loading\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n#gemma_llm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T20:26:26.717705Z","iopub.execute_input":"2024-07-13T20:26:26.718140Z","iopub.status.idle":"2024-07-13T20:28:36.609493Z","shell.execute_reply.started":"2024-07-13T20:26:26.718108Z","shell.execute_reply":"2024-07-13T20:28:36.608203Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Attaching 'model.safetensors' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define a template string with placeholders for 'instruction' and 'response' to structure the output.\ntemplate = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\n# Use the `format` method to substitute the placeholders in the template string with actual values.\n# Here, 'instruction' is set to a question about Kaggle competitions, and 'response' is initially empty.\nprompt = template.format(\n    instruction=\"How do Kaggle competitions work?\",\n    response=\"\",\n)\n\ndisplay(Markdown(gemma_lm.generate(\"How do Kaggle competitions work?\", max_length=256)))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T20:29:20.953953Z","iopub.execute_input":"2024-07-13T20:29:20.954403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning with LoRA (Low-Rank Adaptation)","metadata":{}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=6, #\"rank\" of the LoRA adapters. This controls the size and computational efficiency of the adapters.\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### change rank in aobve and check the difference","metadata":{}},{"cell_type":"markdown","source":"### Applying LoRA to Gemma","metadata":{}},{"cell_type":"code","source":"gemma_lm.backbone.enable_lora(rank=6)\ngemma_lm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/kaggle-docs/questions_answers/data.csv\")\ntrain_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.tail(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess the data","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    df['prompt'] = df[['Category', 'Question', 'Answer']].apply(\n        lambda row: f\"\\n\\nCategory: {row['Category']}\\n\\nQuestion:\\n{row['Question']}\\n\\nAnswer:\\n{row['Answer']}\", axis=1\n    )\n    return df['prompt'].tolist()\n\ndata = preprocess(train_df.copy()) ","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:17:03.473751Z","iopub.execute_input":"2024-07-09T18:17:03.474222Z","iopub.status.idle":"2024-07-09T18:17:03.494729Z","shell.execute_reply.started":"2024-07-09T18:17:03.474181Z","shell.execute_reply":"2024-07-09T18:17:03.493347Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data[59]","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:17:03.496417Z","iopub.execute_input":"2024-07-09T18:17:03.496914Z","iopub.status.idle":"2024-07-09T18:17:03.509955Z","shell.execute_reply.started":"2024-07-09T18:17:03.496872Z","shell.execute_reply":"2024-07-09T18:17:03.508784Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\n\\nCategory: dataset\\n\\nQuestion:\\nWhat are the technical specifications of Kaggle Datasets?\\n\\nAnswer:\\nKaggle Datasets allows you to publish and share datasets privately or publicly. We provide resources for storing and processing datasets, but there are certain technical specifications:\\n\\n* 100GB per dataset limit\\n* 100GB max private datasets (if you exceed this, either make your datasets public or delete unused datasets)\\n* A max of 50 top-level files (if you have more, use a directory structure and upload an archive)\\n\\nWhen you upload a dataset we apply certain processing steps to make the dataset more usable.\\n\\n* A complete archive is created so the dataset can be easily downloaded later\\n* Any archives (e.g., ZIP files) that you upload are uncompressed so that the files are easily accessible in Notebooks (directory structure is preserved)\\n* Data types for tabular data files are automatically detected (e.g., geospatial types)\\n* Column-level metrics are calculated for tabular data which are viewable on the data explorer on the dataset\\'s \"Data\" tab\\n\\nWhen publishing datasets, you might also want to consider the technical specifications of [Notebooks](https://www.kaggle.com/docs/notebooks#technical-specifications) if you intend to use (or encourage other Kaggle users to use) Notebooks to analyze the data.'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine-tuning with LoRA (Low-Rank Adaptation)","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 256 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:17:03.511319Z","iopub.execute_input":"2024-07-09T18:17:03.511795Z","iopub.status.idle":"2024-07-09T18:50:00.241061Z","shell.execute_reply.started":"2024-07-09T18:17:03.511754Z","shell.execute_reply":"2024-07-09T18:50:00.238906Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1975s\u001b[0m 32s/step - loss: 3.0819 - sparse_categorical_accuracy: 0.4644\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f42c3ee1ae0>"},"metadata":{}}]},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\nprompt = template.format(\n    instruction=\"How do Kaggle competitions work?\",\n    response=\"\",\n)\ndisplay(Markdown(gemma_lm.generate(\"How do Kaggle competitions work?\", max_length=256)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference after fine-tuning","metadata":{}},{"cell_type":"markdown","source":"#### Overall, the results are acceptable after fine-tuning but remain incomplete. To address this, a hybrid system incorporating both fine-tuning and RAG could significantly improve the model.","metadata":{}},{"cell_type":"markdown","source":"## Retrieval-Augmented Generation (RAG)","metadata":{}},{"cell_type":"markdown","source":"## Building a Retrieval-Augmented Generation (RAG) Model with Gemma and Langchain","metadata":{}},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes==0.42.0\n!pip3 install -q -U peft==0.8.2\n!pip3 install -q -U trl==0.7.10\n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q -U transformers==4.38.1\n!pip3 install langchain sentence-transformers chromadb langchainhub","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:50:00.243605Z","iopub.execute_input":"2024-07-09T18:50:00.244105Z","iopub.status.idle":"2024-07-09T18:52:29.825947Z","shell.execute_reply.started":"2024-07-09T18:50:00.244068Z","shell.execute_reply":"2024-07-09T18:52:29.824318Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting langchain\n  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting chromadb\n  Downloading chromadb-0.5.3-py3-none-any.whl.metadata (6.8 kB)\nCollecting langchainhub\n  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n  Downloading langchain_core-0.2.12-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.84-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2+cpu)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.60.0)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting orjson>=3.9.12 (from chromadb)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.0)\nCollecting packaging<25,>=23.2 (from langchainhub)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (1.33)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nCollecting urllib3>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain) (2.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nDownloading langchain-0.2.7-py3-none-any.whl (983 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\nDownloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\nDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.12-py3-none-any.whl (355 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.84-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=92711570426973f0606ea80ce3fd9837cbfd42dfc1a12e09c2a3ec43928a7ef2\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, mmh3, urllib3, pyproject_hooks, packaging, orjson, opentelemetry-util-http, humanfriendly, chroma-hnswlib, bcrypt, asgiref, types-requests, coloredlogs, build, posthog, opentelemetry-instrumentation, onnxruntime, langsmith, langchainhub, opentelemetry-instrumentation-asgi, langchain-core, kubernetes, opentelemetry-instrumentation-fastapi, langchain-text-splitters, sentence-transformers, langchain, chromadb\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.1.3 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.5.3 coloredlogs-15.0.1 humanfriendly-10.0 kubernetes-30.1.0 langchain-0.2.7 langchain-core-0.2.12 langchain-text-splitters-0.2.2 langchainhub-0.1.20 langsmith-0.1.84 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.18.1 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.10.6 packaging-24.1 posthog-3.5.0 pypika-0.48.9 pyproject_hooks-1.1.0 sentence-transformers-3.0.1 types-requests-2.32.0.20240622 urllib3-2.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nhf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token \n\n# Login to Hugging Face\nlogin(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:29.828251Z","iopub.execute_input":"2024-07-09T18:52:29.828669Z","iopub.status.idle":"2024-07-09T18:52:30.379619Z","shell.execute_reply.started":"2024-07-09T18:52:29.828608Z","shell.execute_reply":"2024-07-09T18:52:30.378338Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 2. We import the necessary modules for Langchain integration and set up a Hugging Face Endpoint","metadata":{}},{"cell_type":"code","source":"pip install langchain-community","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:30.381218Z","iopub.execute_input":"2024-07-09T18:52:30.381594Z","iopub.status.idle":"2024-07-09T18:52:46.528923Z","shell.execute_reply.started":"2024-07-09T18:52:30.381558Z","shell.execute_reply":"2024-07-09T18:52:46.527078Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.6)\nRequirement already satisfied: langchain<0.3.0,>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.2.7)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.2.12)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.1.84)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (0.2.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (2.5.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (24.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.9.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nDownloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: langchain-community\nSuccessfully installed langchain-community-0.2.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.llms import HuggingFaceEndpoint\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom transformers import AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:46.531487Z","iopub.execute_input":"2024-07-09T18:52:46.532603Z","iopub.status.idle":"2024-07-09T18:52:47.386483Z","shell.execute_reply.started":"2024-07-09T18:52:46.532544Z","shell.execute_reply":"2024-07-09T18:52:47.385115Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### Overall, this code snippet suggests you're setting up a Langchain pipeline to leverage a Hugging Face model for text generation tasks.","metadata":{}},{"cell_type":"code","source":"## Define a custom LangChain\nfrom typing import Any, Optional, List, Mapping\nfrom langchain_core.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:47.402615Z","iopub.execute_input":"2024-07-09T18:52:47.403037Z","iopub.status.idle":"2024-07-09T18:52:47.415070Z","shell.execute_reply.started":"2024-07-09T18:52:47.403002Z","shell.execute_reply":"2024-07-09T18:52:47.413860Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class GemmaLLM(LLM):\n    \"\"\"\n    Custom LangChain LLM class for Keras Gemma model.\n    \"\"\"\n\n    # Class attributes to store the model and desired generation length\n    model: Any = None\n    n: int = None\n\n    def __init__(self, keras_model, n):\n        \"\"\"\n        Constructor for the GemmaLMM class.\n        Args:\n            keras_model: The loaded Keras Gemma model.\n            n: The desired maximum generation length (number of words).\n        \"\"\"\n        super(GemmaLLM, self).__init__()  # Call parent class initialization\n        self.model = keras_model\n        self.n = n\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"\n        Property to identify the LLM type as \"Gemma\".\n        \"\"\"\n        return \"Gemma\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:\n        \"\"\"\n        Core method for generating text using the GemmaLLM.\n\n        Args:\n            prompt: The text prompt or query for generation.\n            stop: Optional list of stop words or phrases (not used here).\n            **kwargs: Additional keyword arguments for generation (not used here).\n\n        Returns:\n            The generated text as a string.\n        \"\"\"\n\n        generated = self.model.generate(prompt, max_length=self.n)\n\n        # Post-processing to extract summarized text (if applicable)\n        split_string = generated.split(\"SUMMARY:\", 1)\n        if len(split_string) > 1:\n            return split_string[1].lstrip('\\n')\n        else:\n            return generated.lstrip('\\n')\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"\n        Property to return identifying parameters of the LLM (n in this case).\n        \"\"\"\n        return {\"n\": self.n}","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:47.416476Z","iopub.execute_input":"2024-07-09T18:52:47.416870Z","iopub.status.idle":"2024-07-09T18:52:47.433076Z","shell.execute_reply.started":"2024-07-09T18:52:47.416836Z","shell.execute_reply":"2024-07-09T18:52:47.431789Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"llm_model = GemmaLLM(gemma_lm, 256)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:47.434746Z","iopub.execute_input":"2024-07-09T18:52:47.435136Z","iopub.status.idle":"2024-07-09T18:52:47.451252Z","shell.execute_reply.started":"2024-07-09T18:52:47.435102Z","shell.execute_reply":"2024-07-09T18:52:47.449680Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### Overall, this custom GemmaLLM class provides a wrapper for the Keras Gemma model, making it compatible with LangChain's LLM framework for text generation tasks.","metadata":{}},{"cell_type":"markdown","source":"#### 3. Chose a dataset","metadata":{}},{"cell_type":"code","source":"# Load a document\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Kaggle\")\ndata = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:47.452947Z","iopub.execute_input":"2024-07-09T18:52:47.453374Z","iopub.status.idle":"2024-07-09T18:52:48.517954Z","shell.execute_reply.started":"2024-07-09T18:52:47.453329Z","shell.execute_reply":"2024-07-09T18:52:48.516655Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from langchain_community.document_loaders import TextLoader\nfrom langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_text_splitters import CharacterTextSplitter","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:48.519585Z","iopub.execute_input":"2024-07-09T18:52:48.520209Z","iopub.status.idle":"2024-07-09T18:52:48.551094Z","shell.execute_reply.started":"2024-07-09T18:52:48.520168Z","shell.execute_reply":"2024-07-09T18:52:48.549732Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Initialize a CharacterTextSplitter to split text into chunks of 1000 characters without overlap\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n\n# Assuming 'data' contains the text to be split, split it into manageable chunks using the text splitter\ndocs = text_splitter.split_documents(data)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:48.552525Z","iopub.execute_input":"2024-07-09T18:52:48.552955Z","iopub.status.idle":"2024-07-09T18:52:48.560321Z","shell.execute_reply.started":"2024-07-09T18:52:48.552916Z","shell.execute_reply":"2024-07-09T18:52:48.558724Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"pip install langchain-huggingface","metadata":{"execution":{"iopub.status.busy":"2024-07-08T18:52:44.001529Z","iopub.execute_input":"2024-07-08T18:52:44.002020Z","iopub.status.idle":"2024-07-08T18:53:08.065583Z","shell.execute_reply.started":"2024-07-08T18:52:44.001985Z","shell.execute_reply":"2024-07-08T18:53:08.064038Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting langchain-huggingface\n  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.23.2)\nRequirement already satisfied: langchain-core<0.3,>=0.1.52 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.2.11)\nRequirement already satisfied: sentence-transformers>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (3.0.1)\nCollecting tokenizers>=0.19.1 (from langchain-huggingface)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting transformers>=4.39.0 (from langchain-huggingface)\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.84)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.5.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.2.3)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.11.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (9.5.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.6)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\nDownloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\nDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, langchain-huggingface\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed langchain-huggingface-0.0.3 tokenizers-0.19.1 transformers-4.42.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:48.562109Z","iopub.execute_input":"2024-07-09T18:52:48.562492Z","iopub.status.idle":"2024-07-09T18:52:49.347112Z","shell.execute_reply.started":"2024-07-09T18:52:48.562460Z","shell.execute_reply":"2024-07-09T18:52:49.345185Z"},"trusted":true},"execution_count":29,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_huggingface'"],"ename":"ModuleNotFoundError","evalue":"No module named 'langchain_huggingface'","output_type":"error"}]},{"cell_type":"code","source":"import sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-09T19:28:16.898679Z","iopub.execute_input":"2024-07-09T19:28:16.900787Z","iopub.status.idle":"2024-07-09T19:28:19.054237Z","shell.execute_reply.started":"2024-07-09T19:28:16.900709Z","shell.execute_reply":"2024-07-09T19:28:19.052924Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:533: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n  import torch._dynamo as dynamo  # noqa: F401\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_function = SentenceTransformerEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T18:52:49.351245Z","iopub.status.idle":"2024-07-09T18:52:49.351745Z","shell.execute_reply.started":"2024-07-09T18:52:49.351497Z","shell.execute_reply":"2024-07-09T18:52:49.351517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize SentenceTransformerEmbeddings with a specific model to create embeddings for text chunks\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Create a Chroma database to store embeddings of the text chunks, using the specified embedding function\ndb = Chroma.from_documents(docs, embedding_function)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T19:28:35.801527Z","iopub.execute_input":"2024-07-09T19:28:35.802101Z","iopub.status.idle":"2024-07-09T19:28:39.289097Z","shell.execute_reply.started":"2024-07-09T19:28:35.802057Z","shell.execute_reply":"2024-07-09T19:28:39.287459Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  warn_deprecated(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22024628b20544f9a18a026ceb1f3161"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ceeafb5531d455ea0a3818ef4f66669"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19534d7bf701419aad8efa32d5215b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e7ed051b4f4c9b85265ef60c9aaef2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"988697075ab94af9a349552c74b944d3"}},"metadata":{}},{"name":"stdout","text":"Unexpected exception formatting exception. Falling back to standard exception\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1535, in _get_module\n  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'transformers.models.cohere.configuration_cohere'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_33/2194975298.py\", line 2, in <module>\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\n    return wrapped(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 79, in __init__\n    self.client = sentence_transformers.SentenceTransformer(\n  File \"/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 287, in __init__\n    modules = self._load_sbert_model(\n  File \"/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 1487, in _load_sbert_model\n    module = Transformer(model_name_or_path, cache_dir=cache_folder, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 54, in __init__\n    self._load_model(model_name_or_path, config, cache_dir, **model_args)\n  File \"/opt/conda/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 85, in _load_model\n    self.auto_model = AutoModel.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 540, in from_pretrained\n    trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 751, in keys\n    for key, name in self._config_mapping.items()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 752, in <listcomp>\n    if key in self._model_mapping.keys()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 748, in _load_attr_from_module\n    def keys(self):\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 692, in getattribute_from_module\n    # Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1525, in __getattr__\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1537, in _get_module\nRuntimeError: Failed to import transformers.models.cohere.configuration_cohere because of the following error (look up to see its traceback):\nNo module named 'transformers.models.cohere.configuration_cohere'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n    frames.append(self.format_record(record))\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n    frame_info.lines, Colors, self.has_colors, lvals\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n    return self._sd.lines\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n    pieces = self.included_pieces\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n    return only(\n  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\", line 116, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain.chains import RetrievalQA","metadata":{"execution":{"iopub.status.busy":"2024-07-08T19:37:14.190601Z","iopub.execute_input":"2024-07-08T19:37:14.192437Z","iopub.status.idle":"2024-07-08T19:37:14.276882Z","shell.execute_reply.started":"2024-07-08T19:37:14.192380Z","shell.execute_reply":"2024-07-08T19:37:14.275823Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Configure the retriever using the Chroma vector store `db`, setting the search type to 'mmr' and defining search parameters\nretriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 20})\n\n# Pull a RAG prompt configuration from the Langchain hub\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# Define a function to format the retrieved documents, concatenating them with double newlines\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Create a RAG chain by composing different components:\n# `retriever | format_docs` creates the context by fetching and formatting documents\n# `RunnablePassthrough()` passes the question directly as is\n# `prompt` applies the RAG prompt configuration\n# `llm` represents the language model to generate answers (not defined in this snippet)\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm_model\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rag_chain.invoke(\"Who own Kaggle?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the ConversationBufferMemory class from the langchain.memory module.\nfrom langchain.memory import ConversationBufferMemory\n\n# Import the ConversationalRetrievalChain class from the langchain.chains module.\nfrom langchain.chains import ConversationalRetrievalChain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a ConversationBufferMemory object with a specific key for storing chat history.\n# The 'return_messages' flag is set to True to ensure that the stored messages are retrievable.\nmemory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n\n# Define a string that specifies how the prompt should be structured for rephrasing follow-up questions.\n# This template includes placeholders for chat history and the follow-up question that will be rephrased.\ncustom_template = \"\"\"Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question, in its original English.\n                        Chat History:\n                        {chat_history}\n                        Follow-Up Input: {question}\n                        Standalone question:\"\"\"\n\n# Create a PromptTemplate object using the custom template string.\n# This object will be used to format the input for the language model.\nfrom langchain.prompts import PromptTemplate  # Importing PromptTemplate, assuming it's necessary and was omitted in the original code\nCUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n\n# Instantiate a ConversationalRetrievalChain object by specifying the language model (llm),\n# the type of chain, a retriever component (assumed to be part of a database 'db'), the memory component,\n# and a custom prompt template for question condensation.\n# This creates a chain that can handle conversational context and retrieval-based tasks.\nconversational_chain = ConversationalRetrievalChain.from_llm(\n    llm=llm_model,  \n    chain_type=\"stuff\",  # 'stuff' should be replaced with the actual type of chain intended to use\n    retriever=db.as_retriever(),  # Assuming 'db' is a predefined database object with a method to create a retriever\n    memory=memory,\n    condense_question_prompt=CUSTOM_QUESTION_PROMPT\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prompt Engineering","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"\"\"\n**Act as an expert Kaggle assistant.**\n\n* **Focus on Actionable Insights:**  Provide direct answers, links to Kaggle resources, or suggest search terms that will immediately help me with my Kaggle tasks.\n* **Understand My Intent:** If a question is ambiguous, ask for clarifications or provide examples to help me rephrase it effectively. \n* **Be Proactive:**  Offer guidance even if I don't have a precise question. Suggest popular datasets, trending competitions, or widely-used techniques based on my interests.  \n* **Stay Concise:**  Deliver the most important information first.\n**Let's begin! What's your Kaggle-related question or task?** \n\"\"\" ,\n    response=\"\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Markdown(gemma_lm.generate(prompt, max_length=256)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Markdown(gemma_lm.generate(\"Introduce me to the Kaggle platform.\", max_length=256)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}